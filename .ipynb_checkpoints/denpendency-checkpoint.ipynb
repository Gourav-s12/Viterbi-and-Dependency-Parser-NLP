{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YyYutpcytWo_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GaMAmZjiVoiV"
   },
   "outputs": [],
   "source": [
    "# reading dataset \n",
    "def parse_dataset(file_path):\n",
    "    dataset = []\n",
    "    current_sentence = None\n",
    "    dependacy = None\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('# sent_id'):\n",
    "                # Start of a new sentence\n",
    "                if current_sentence:\n",
    "                    current_sentence['dependacy'] = dependacy\n",
    "                    dataset.append(current_sentence)\n",
    "                current_sentence = {'sent_id': line.split('=')[1].strip(), 'text': ''}\n",
    "            elif line.startswith('# text'):\n",
    "                # Text of the sentence\n",
    "                current_sentence['text'] = line.split('=')[1].strip()\n",
    "                current_sentence['pos_tag'] = []\n",
    "                dependacy = dict()\n",
    "            elif line:\n",
    "                # Details about each word in the sentence\n",
    "                parts = line.split()\n",
    "                if '-' in parts[0] or '.' in parts[0]:\n",
    "                    continue\n",
    "\n",
    "                head_value = parts[4] if parts[4] != '_' else None\n",
    "\n",
    "                parts[0] = int(parts[0]) if parts[0] is not None else None\n",
    "\n",
    "                dependacy[parts[0]] = {\n",
    "                        'index': parts[0],\n",
    "                        'original_word': parts[1],\n",
    "                        'word': parts[2],\n",
    "                        'pos': parts[3],\n",
    "                        'head': int(head_value) if head_value is not None else None,\n",
    "                        'dependency_relation': parts[5]\n",
    "                    }\n",
    "                current_sentence['pos_tag'].append(parts[3])\n",
    "\n",
    "    # Append the last sentence to the dataset\n",
    "    if current_sentence:\n",
    "        current_sentence['dependacy'] = dependacy\n",
    "        dataset.append(current_sentence)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MS6Ovc7tc4vf",
    "outputId": "be64372a-bc35-4601-90f4-810af8406ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent_id': 'GUM_academic_art-1', 'text': 'Aesthetic Appreciation and Spanish Art:', 'pos_tag': ['JJ', 'NN', 'CC', 'JJ', 'NN', ':'], 'dependacy': {1: {'index': 1, 'original_word': 'Aesthetic', 'word': 'aesthetic', 'pos': 'JJ', 'head': 2, 'dependency_relation': 'amod'}, 2: {'index': 2, 'original_word': 'Appreciation', 'word': 'appreciation', 'pos': 'NN', 'head': 0, 'dependency_relation': 'root'}, 3: {'index': 3, 'original_word': 'and', 'word': 'and', 'pos': 'CC', 'head': 5, 'dependency_relation': 'cc'}, 4: {'index': 4, 'original_word': 'Spanish', 'word': 'Spanish', 'pos': 'JJ', 'head': 5, 'dependency_relation': 'amod'}, 5: {'index': 5, 'original_word': 'Art', 'word': 'art', 'pos': 'NN', 'head': 2, 'dependency_relation': 'conj'}, 6: {'index': 6, 'original_word': ':', 'word': ':', 'pos': ':', 'head': 2, 'dependency_relation': 'punct'}}}\n",
      "{'sent_id': 'GUM_academic_art-2', 'text': 'Insights from Eye-Tracking', 'pos_tag': ['NNS', 'IN', 'NN', 'HYPH', 'NN'], 'dependacy': {1: {'index': 1, 'original_word': 'Insights', 'word': 'insight', 'pos': 'NNS', 'head': 0, 'dependency_relation': 'root'}, 2: {'index': 2, 'original_word': 'from', 'word': 'from', 'pos': 'IN', 'head': 5, 'dependency_relation': 'case'}, 3: {'index': 3, 'original_word': 'Eye', 'word': 'eye', 'pos': 'NN', 'head': 5, 'dependency_relation': 'compound'}, 4: {'index': 4, 'original_word': '-', 'word': '-', 'pos': 'HYPH', 'head': 3, 'dependency_relation': 'punct'}, 5: {'index': 5, 'original_word': 'Tracking', 'word': 'tracking', 'pos': 'NN', 'head': 1, 'dependency_relation': 'nmod'}}}\n",
      "{'sent_id': 'GUM_academic_art-3', 'text': 'Claire Bailey-Ross claire.bailey-ross@port.ac.uk University of Portsmouth, United Kingdom', 'pos_tag': ['NNP', 'NNP', 'HYPH', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', ',', 'NNP', 'NNP'], 'dependacy': {1: {'index': 1, 'original_word': 'Claire', 'word': 'Claire', 'pos': 'NNP', 'head': 0, 'dependency_relation': 'root'}, 2: {'index': 2, 'original_word': 'Bailey', 'word': 'Bailey', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'flat'}, 3: {'index': 3, 'original_word': '-', 'word': '-', 'pos': 'HYPH', 'head': 4, 'dependency_relation': 'punct'}, 4: {'index': 4, 'original_word': 'Ross', 'word': 'Ross', 'pos': 'NNP', 'head': 2, 'dependency_relation': 'flat'}, 5: {'index': 5, 'original_word': 'claire.bailey-ross@port.ac.uk', 'word': 'claire.bailey-ross@port.ac.uk', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'list'}, 6: {'index': 6, 'original_word': 'University', 'word': 'University', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'list'}, 7: {'index': 7, 'original_word': 'of', 'word': 'of', 'pos': 'IN', 'head': 8, 'dependency_relation': 'case'}, 8: {'index': 8, 'original_word': 'Portsmouth', 'word': 'Portsmouth', 'pos': 'NNP', 'head': 6, 'dependency_relation': 'nmod'}, 9: {'index': 9, 'original_word': ',', 'word': ',', 'pos': ',', 'head': 11, 'dependency_relation': 'punct'}, 10: {'index': 10, 'original_word': 'United', 'word': 'Unite', 'pos': 'NNP', 'head': 11, 'dependency_relation': 'amod'}, 11: {'index': 11, 'original_word': 'Kingdom', 'word': 'Kingdom', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'list'}}}\n",
      "{'sent_id': 'GUM_academic_art-4', 'text': 'Andrew Beresford a.m.beresford@durham.ac.uk Durham University, United Kingdom', 'pos_tag': ['NNP', 'NNP', 'NNP', 'NNP', 'NNP', ',', 'NNP', 'NNP'], 'dependacy': {1: {'index': 1, 'original_word': 'Andrew', 'word': 'Andrew', 'pos': 'NNP', 'head': 0, 'dependency_relation': 'root'}, 2: {'index': 2, 'original_word': 'Beresford', 'word': 'Beresford', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'flat'}, 3: {'index': 3, 'original_word': 'a.m.beresford@durham.ac.uk', 'word': 'a.m.beresford@durham.ac.uk', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'list'}, 4: {'index': 4, 'original_word': 'Durham', 'word': 'Durham', 'pos': 'NNP', 'head': 5, 'dependency_relation': 'compound'}, 5: {'index': 5, 'original_word': 'University', 'word': 'University', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'list'}, 6: {'index': 6, 'original_word': ',', 'word': ',', 'pos': ',', 'head': 8, 'dependency_relation': 'punct'}, 7: {'index': 7, 'original_word': 'United', 'word': 'Unite', 'pos': 'NNP', 'head': 8, 'dependency_relation': 'amod'}, 8: {'index': 8, 'original_word': 'Kingdom', 'word': 'Kingdom', 'pos': 'NNP', 'head': 1, 'dependency_relation': 'list'}}}\n"
     ]
    }
   ],
   "source": [
    "# reading dataset train\n",
    "file_path = 'train.txt'\n",
    "parsed_dataset_train = parse_dataset(file_path)\n",
    "\n",
    "# Print the parsed dataset for inspection\n",
    "for sentence in parsed_dataset_train[:4]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVhgb6AlDfzA",
    "outputId": "d2b32cef-4040-421d-fbc1-5534cfb865ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text ['How', 'do', 'people', 'look', 'at', 'and', 'experience', 'art', '?']\n",
      "Pos Tag ['WRB', 'VBP', 'NNS', 'VB', 'IN', 'CC', 'VB', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "# get text in a sentence\n",
    "def get_text(sentence_data):\n",
    "  return [ row['original_word'] for row in sentence_data['dependacy'].values()]\n",
    "\n",
    "print(f\"Original text {get_text(parsed_dataset_train[6])}\")\n",
    "print(f\"Pos Tag {parsed_dataset_train[6]['pos_tag']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration class and oracle\n",
    "class Configuration:\n",
    "    def __init__(self, sentence_info):\n",
    "        self.sentence_info = sentence_info\n",
    "        self.original_sentence = \" \".join(get_text(sentence_info))\n",
    "        self.dependency = sentence_info['dependacy']\n",
    "        self.stack = []\n",
    "        self.buffer = list(sentence_info['dependacy'].values())   # get_text(sentence_info)\n",
    "        self.arcs = []\n",
    "\n",
    "    def do_left_arc(self):\n",
    "        if len(self.stack) < 1 and len(self.buffer) < 1:\n",
    "            raise ValueError(\"Cannot perform LEFT-ARC: Stack or Buffer has fewer than 1 item\")\n",
    "        dependent = self.stack.pop()\n",
    "        head = self.buffer[0]\n",
    "        self.arcs.append((head, dependent))\n",
    "\n",
    "    def do_right_arc(self):\n",
    "        if len(self.stack) < 1 and len(self.buffer) < 1:\n",
    "            raise ValueError(\"Cannot perform RIGHT-ARC: Stack or Buffer has fewer than 1 item\")\n",
    "        head = self.stack[-1]\n",
    "        dependent = self.buffer.pop(0)\n",
    "        self.stack.append(dependent)\n",
    "        self.arcs.append((head, dependent))\n",
    "\n",
    "    def do_reduce(self):\n",
    "        if len(self.stack) < 1:\n",
    "            raise ValueError(\"Cannot perform REDUCE: Stack is empty\")\n",
    "        head = self.stack[-1]\n",
    "        for arc in self.arcs:\n",
    "            if arc[1]['original_word'] == head['original_word']:\n",
    "                self.stack.pop()\n",
    "                return\n",
    "        raise ValueError(\"Cannot perform REDUCE: stack top head is still unknown\")\n",
    "\n",
    "    def do_shift(self):\n",
    "        if len(self.buffer) < 1:\n",
    "            raise ValueError(\"Cannot perform SHIFT: Buffer is empty\")\n",
    "        word = self.buffer.pop(0)\n",
    "        self.stack.append(word)\n",
    "\n",
    "    def get_head(self, text):\n",
    "        for v in self.dependency.values():\n",
    "            if v['head'] == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return self.dependency[v['head']]['original_word']\n",
    "\n",
    "    def curr_config(self):\n",
    "        print(\"\\nCurrent stack:\", [ row['original_word'] for row in self.stack])\n",
    "        print(\"Current buffer:\", [ row['original_word'] for row in self.buffer])\n",
    "        print(\"Current arcs:\", [ (head['original_word'], dep['original_word']) for head, dep in self.arcs])\n",
    "\n",
    "    # get oracle transaction\n",
    "    def oracle_transition(self):\n",
    "        #if buffer is empty\n",
    "        if len(self.buffer) == 0:\n",
    "            return \"END\"\n",
    "        \n",
    "        # if stack empty \n",
    "        if len(self.stack) == 0:\n",
    "            return \"SHIFT\"\n",
    "            \n",
    "        # Rule 1: LEFT-ARC\n",
    "        if self.buffer and self.stack:\n",
    "            top_s = self.stack[-1]\n",
    "            first_b = self.buffer[0]\n",
    "            # top_s -> first_b and not top_s -> any in current arc \n",
    "            if first_b['index'] == top_s['head'] and \\\n",
    "               (top_s['head'] != 0 or not any(dep['index'] == top_s['index'] for head, dep in self.arcs)):\n",
    "                return \"LEFT-ARC\"\n",
    "\n",
    "        # Rule 2: RIGHT-ARC\n",
    "        if self.buffer and self.stack:\n",
    "            top_s = self.stack[-1]\n",
    "            first_b = self.buffer[0]\n",
    "            # top_s <- first_b\n",
    "            if top_s['index'] == first_b['head']:\n",
    "                return \"RIGHT-ARC\"\n",
    "\n",
    "        # Rule 3: REDUCE\n",
    "        if self.stack:\n",
    "            top_s = self.stack[-1]\n",
    "            first_b = self.buffer[0]\n",
    "            # dependency_list = [ (row['head'],row['index']) for row in self.dependency.values() if row['head'] != 0 ]\n",
    "            arc = [ (head['index'],dep['index']) for  head, dep in self.arcs ]\n",
    "            # w -> first_b or w <- first_b and w != top_s and top_s <- any in current arc \n",
    "            if any(head['index'] == top_s['head'] for head, dep in self.arcs):\n",
    "                for word in self.stack:\n",
    "                    if word != top_s:\n",
    "                        # if (word['index'], first_b['index']) in dependency_list or \\\n",
    "                        #    (first_b['index'], word['index']) in dependency_list:\n",
    "                        if ((word['index'], first_b['index']) in arc) or ((first_b['index'], word['index']) in arc):\n",
    "                            return \"REDUCE\"\n",
    "\n",
    "        # Rule 4: SHIFT\n",
    "        # if nothing works shift\n",
    "        return \"SHIFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current stack: []\n",
      "Current buffer: ['Aesthetic', 'Appreciation', 'and', 'Spanish', 'Art', ':']\n",
      "Current arcs: []\n",
      "SHIFT\n",
      "LEFT-ARC\n",
      "SHIFT\n",
      "SHIFT\n",
      "SHIFT\n",
      "LEFT-ARC\n",
      "LEFT-ARC\n",
      "RIGHT-ARC\n",
      "SHIFT\n",
      "RIGHT-ARC\n",
      "END\n",
      "\n",
      "Current stack: ['Appreciation', ':']\n",
      "Current buffer: []\n",
      "Current arcs: [('Appreciation', 'Aesthetic'), ('Art', 'Spanish'), ('Art', 'and'), ('Appreciation', 'Art'), ('Appreciation', ':')]\n"
     ]
    }
   ],
   "source": [
    "# testing:\n",
    "# sentence_info = {'sent_id': 'GUM_academic_art-1', 'text': 'Aesthetic Appreciation and Spanish Art:', 'pos_tag': ['JJ', 'NN', 'CC', 'JJ', 'NN', ':'], 'dependacy': {'1': {'original_word': 'Aesthetic', 'word': 'aesthetic', 'pos': 'JJ', 'head': 2, 'dependency_relation': 'amod'}, '2': {'original_word': 'Appreciation', 'word': 'appreciation', 'pos': 'NN', 'head': 0, 'dependency_relation': 'root'}, '3': {'original_word': 'and', 'word': 'and', 'pos': 'CC', 'head': 5, 'dependency_relation': 'cc'}, '4': {'original_word': 'Spanish', 'word': 'Spanish', 'pos': 'JJ', 'head': 5, 'dependency_relation': 'amod'}, '5': {'original_word': 'Art', 'word': 'art', 'pos': 'NN', 'head': 2, 'dependency_relation': 'conj'}, '6': {'original_word': ':', 'word': ':', 'pos': ':', 'head': 2, 'dependency_relation': 'punct'}}}\n",
    "sentence_info = parsed_dataset_train[0]\n",
    "config = Configuration(sentence_info)\n",
    "\n",
    "config.curr_config()\n",
    "# Perform transitions\n",
    "print(config.oracle_transition())\n",
    "config.do_shift()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_left_arc()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_shift()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_shift()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_shift()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_left_arc()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_left_arc()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_right_arc()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_reduce()\n",
    "print(config.oracle_transition())\n",
    "# config.curr_config()\n",
    "config.do_right_arc()\n",
    "print(config.oracle_transition())\n",
    "config.curr_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EhBbKLXb5rq8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# pos_tag count for P_size\n",
    "# Initialize a Counter to store the frequency of each POS tag\n",
    "pos_tag_frequency = Counter()\n",
    "\n",
    "# Iterate through the dataset to collect unique POS tags and their frequency counts\n",
    "for sentence_data in parsed_dataset_train:\n",
    "    pos_tags = sentence_data['pos_tag']\n",
    "    pos_tag_frequency.update(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HZYxgXO5ymW",
    "outputId": "cb617aec-5016-4652-ac32-0c6a8ae52fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique POS tags: 46\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of unique POS tags\n",
    "print(\"Total unique POS tags:\", len(pos_tag_frequency))\n",
    "# Print the unique POS tags and their frequency counts\n",
    "# print(\"Unique POS tags and their frequency counts:\")\n",
    "# for tag, count in pos_tag_frequency.items():\n",
    "#     print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g-noZ9nVCYkj"
   },
   "outputs": [],
   "source": [
    "# normalized_tokens count for V_size\n",
    "unique_words= Counter()\n",
    "# Iterate through the dataset to collect unique POS tags\n",
    "for sentence_data in parsed_dataset_train:\n",
    "    words = get_text(sentence_data)\n",
    "    unique_words.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJwHd25dCaA_",
    "outputId": "39082285-32e8-4398-ce33-29b79422fed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 6606\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of unique POS tags\n",
    "print(\"Total unique words:\", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized_tokens count for V_size\n",
    "total_sentence_in_training = len(parsed_dataset_train)\n",
    "# Get normalized tokens\n",
    "normalized_tokens = [(k, v) for k, v in unique_words.items() if v < total_sentence_in_training]\n",
    "# Sort based on frequency of occurrence (v)\n",
    "normalized_tokens = sorted(normalized_tokens, key=lambda x: x[1], reverse=True)[:1000]\n",
    "# Extract only the tokens without frequencies\n",
    "normalized_tokens = [token for token, _ in normalized_tokens]\n",
    "len(normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency_relation count for R_size\n",
    "Dependency_relation = set()\n",
    "root_list = []\n",
    "# Iterate through the dataset to collect unique POS tags and their frequency counts\n",
    "for sentence_data in parsed_dataset_train:\n",
    "    for v in sentence_data['dependacy'].values():\n",
    "        if v['dependency_relation'] == 'root':\n",
    "            root_list.append(v['original_word'])\n",
    "        Dependency_relation.add(v['dependency_relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dependency relation: 52\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of Dependency_relation\n",
    "print(\"Total Dependency relation:\", len(Dependency_relation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(conf, transition, vocabulary = normalized_tokens, pos_tags = pos_tag_frequency, dependency_relations = Dependency_relation):\n",
    "    # Initialize the feature vector with zeros\n",
    "    pos_tag = list(pos_tags.keys())\n",
    "    dependency_relation = list(dependency_relations)\n",
    "    V_size = len(vocabulary)\n",
    "    R_size = len(dependency_relation)\n",
    "    P_size = len(pos_tag)\n",
    "    \n",
    "    feature_vector_size = 4 * (2 * V_size + 3 * P_size + 4 * R_size)\n",
    "    f = np.zeros(feature_vector_size, dtype=int)\n",
    "    S, B, A = conf.stack, conf.buffer, conf.arcs\n",
    "    \n",
    "    # Helper function to update feature vector for a given feature index \n",
    "    def set_feature (idx, value=1):\n",
    "        if 0 <= idx < feature_vector_size:\n",
    "            f[idx] = value\n",
    "\n",
    "    def update_top_S_DEP(top_S, offset):\n",
    "        head_index = top_S['index']\n",
    "        if head_index != 0:  # If head is not root\n",
    "            dep_relation = top_S['dependency_relation']\n",
    "            # Update TOP.DEP\n",
    "            dep_index = offset + V_size + P_size + dependency_relation.index(dep_relation)\n",
    "            # print(\"top_dep\")\n",
    "            set_feature(dep_index)\n",
    "\n",
    "            # Update TOP.LDEP\n",
    "            left_dep = get_left_most_dependency(head_index)\n",
    "            if left_dep:\n",
    "                left_dep_relation = left_dep['dependency_relation']\n",
    "                left_dep_index = offset + V_size + P_size + R_size + dependency_relation.index(left_dep_relation)\n",
    "                # print(\"top_l_dep\")\n",
    "                set_feature(left_dep_index)\n",
    "\n",
    "            # Update TOP.RDEP\n",
    "            right_dep = get_right_most_dependency(head_index)\n",
    "            if right_dep:\n",
    "                right_dep_relation = right_dep['dependency_relation']\n",
    "                right_dep_index = offset + V_size + P_size + 2 * R_size + dependency_relation.index(right_dep_relation)\n",
    "                # print(\"top_r_dep\")\n",
    "                set_feature(right_dep_index)\n",
    "    \n",
    "    def update_buffer_DEP(first_B, offset):\n",
    "        head_index = first_B['index']\n",
    "        if head_index != 0:  # If head is not root\n",
    "            # Update FIRST.LDEP\n",
    "            left_dep = get_left_most_dependency(head_index)\n",
    "            if left_dep:\n",
    "                left_dep_relation = left_dep['dependency_relation']\n",
    "                left_dep_index = offset + 2 * V_size + 2 * P_size + 3 * R_size + dependency_relation.index(left_dep_relation)\n",
    "                # print(\"1st_l_dep\")\n",
    "                set_feature(left_dep_index)\n",
    "    \n",
    "    def get_left_most_dependency(head_index):\n",
    "        left_most = None\n",
    "        for arc in A:\n",
    "            if arc[1]['head'] == head_index:\n",
    "                if not left_most or arc[1]['index'] < left_most['index']:\n",
    "                    left_most = arc[1]\n",
    "        return left_most\n",
    "    \n",
    "    def get_right_most_dependency(head_index):\n",
    "        right_most = None\n",
    "        for arc in A:\n",
    "            if arc[1]['head'] == head_index:\n",
    "                if not right_most or arc[1]['index'] > right_most['index']:\n",
    "                    right_most = arc[1]\n",
    "        return right_most\n",
    "\n",
    "    \n",
    "    # Feature indices based on the transition \n",
    "    transition_offset = {'LEFT-ARC': 0, 'RIGHT-ARC': 1, 'REDUCE': 2, 'SHIFT': 3}[transition] * (2 * V_size + 3 * P_size + 4 * R_size)\n",
    "    \n",
    "    # TOP feature\n",
    "    if S:\n",
    "        top_S = S[-1]  # Last stack item is the top\n",
    "        # TOP\n",
    "        top_token = top_S['original_word']  # Normalized token\n",
    "        if top_token in vocabulary:\n",
    "            # print(\"top_voc\")\n",
    "            set_feature(vocabulary.index(top_token) + transition_offset)\n",
    "        \n",
    "        # TOP.POS\n",
    "        top_POS = top_S['pos']  # POS_tag for TOS\n",
    "        if top_POS in pos_tag:\n",
    "            # print(\"top_pos\")\n",
    "            set_feature(V_size + pos_tag.index(top_POS) + transition_offset)\n",
    "        \n",
    "        update_top_S_DEP(top_S, transition_offset)\n",
    "        \n",
    "    # FIRST feature\n",
    "    if B:\n",
    "        first_B = B[0]  # First buffer item\n",
    "        # FIRST\n",
    "        first_token = first_B['original_word']  # Normalized token\n",
    "        if first_token in vocabulary:\n",
    "            # print(\"1st_voc\")\n",
    "            set_feature(V_size + P_size + 3 * R_size + vocabulary.index(first_token) + transition_offset)\n",
    "        \n",
    "        # FIRST.POS\n",
    "        first_POS = first_B['pos']  # POS_tag for first (Buffer)\n",
    "        if first_POS in pos_tag:\n",
    "            # print(\"1st_pos\")\n",
    "            set_feature(2 * V_size + P_size + 3 * R_size + pos_tag.index(first_POS) + transition_offset)\n",
    "        \n",
    "        update_buffer_DEP(first_B, transition_offset)\n",
    "    \n",
    "    # LOOK.POS\n",
    "    if len(B) >= 2:\n",
    "        second_pos = B[1]\n",
    "        look_POS = second_pos['pos']\n",
    "        if look_POS in pos_tag:\n",
    "            # print(\"2nd_pos\")\n",
    "            set_feature(2 * V_size + 2 * P_size + 4 * R_size + pos_tag.index(look_POS) + transition_offset)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# to generate training data\n",
    "def get_training_data(sentences):\n",
    "    training_instances = []\n",
    "    error = 0\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        # Initial configuration for the sentence\n",
    "        config = Configuration(sentence)\n",
    "        is_this_sentence_error = False\n",
    "        training_instances_per_sentence = []\n",
    "        while len(config.buffer) >= 1: # Continue until buffer is empty\n",
    "            # Determine the gold-standard action using the oracle\n",
    "            gold_action = config.oracle_transition()\n",
    "            # Encode current configuration and gold action into feature vector\n",
    "            training_instances_per_sentence.append((copy.deepcopy(config), gold_action))\n",
    "            # Update configuration based on the gold action\n",
    "            try:\n",
    "                if gold_action == 'LEFT-ARC':\n",
    "                    config.do_left_arc()\n",
    "                elif gold_action == 'RIGHT-ARC':\n",
    "                    config.do_right_arc()\n",
    "                elif gold_action == 'REDUCE':\n",
    "                    config.do_reduce()\n",
    "                elif gold_action == 'SHIFT':\n",
    "                    config.do_shift()\n",
    "                else:\n",
    "                    print(\"Invalid action for sentence index:\", index)\n",
    "                    print(\"Invalid action:\", gold_action)\n",
    "                    break\n",
    "            except:\n",
    "                error += 1\n",
    "                # print(index)\n",
    "                is_this_sentence_error = True # just to ensure there is no error in original gold tag data\n",
    "                break\n",
    "        if not is_this_sentence_error:\n",
    "            training_instances.extend(training_instances_per_sentence)\n",
    "    return training_instances, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, error = get_training_data(parsed_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data = 54176\n",
      "number of error = 0 out of 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of training data = {len(train_data)}\")\n",
    "print(f\"number of error = {error} out of {len(parsed_dataset_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "# to train the weights\n",
    "def training_classifier(train_data, epochs, vocabulary = normalized_tokens, pos_tags = pos_tag_frequency, dependency_relations = Dependency_relation):\n",
    "    # Initialize weights with a vector of all 1s\n",
    "    pos_tag = list(pos_tags.keys())\n",
    "    dependency_relation = list(dependency_relations)\n",
    "    V_size = len(vocabulary)\n",
    "    R_size = len(dependency_relation)\n",
    "    P_size = len(pos_tag)\n",
    "    \n",
    "    feature_vector_size = 4 * (2 * V_size + 3 * P_size + 4 * R_size)  # Assuming all configurations have the same feature vector size\n",
    "    w = np.ones(feature_vector_size)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # random.shuffle(train_data) # we can on this to get better acuuracy\n",
    "        for config, gold_action in train_data:\n",
    "            \n",
    "            t_star = None\n",
    "            max_score = float('-inf')\n",
    "            for transition in ['LEFT-ARC', 'RIGHT-ARC', 'REDUCE', 'SHIFT']:\n",
    "                score = get_score(config, transition, w)\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    t_star = transition\n",
    "            \n",
    "            t_gold = gold_action  # Gold-standard action\n",
    "            # update w if gold and star is not same\n",
    "            if t_star != t_gold:\n",
    "                w += get_feature_vector(config, t_gold) - get_feature_vector(config, t_star)\n",
    "                \n",
    "    return w\n",
    "\n",
    "def get_score(config, transition, weights):\n",
    "    # Calculate the score for a given transition and configuration\n",
    "    feature_vector = get_feature_vector(config, transition)\n",
    "    return np.dot(weights, feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:03<00:00, 12.67s/it]\n"
     ]
    }
   ],
   "source": [
    "w = training_classifier(train_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 4265,\n",
       "         0.0: 889,\n",
       "         2.0: 783,\n",
       "         3.0: 607,\n",
       "         -1.0: 605,\n",
       "         4.0: 423,\n",
       "         -2.0: 423,\n",
       "         -3.0: 279,\n",
       "         5.0: 263,\n",
       "         -4.0: 176,\n",
       "         6.0: 167,\n",
       "         -5.0: 82,\n",
       "         7.0: 73,\n",
       "         8.0: 64,\n",
       "         -6.0: 63,\n",
       "         -7.0: 41,\n",
       "         9.0: 34,\n",
       "         10.0: 33,\n",
       "         -8.0: 23,\n",
       "         11.0: 17,\n",
       "         12.0: 14,\n",
       "         -9.0: 13,\n",
       "         -10.0: 12,\n",
       "         15.0: 10,\n",
       "         13.0: 5,\n",
       "         -11.0: 4,\n",
       "         -12.0: 4,\n",
       "         17.0: 3,\n",
       "         14.0: 2,\n",
       "         -13.0: 2,\n",
       "         18.0: 2,\n",
       "         16.0: 1,\n",
       "         -16.0: 1,\n",
       "         -15.0: 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector weight stored in 'dependency_model_on.npy'\n"
     ]
    }
   ],
   "source": [
    "# Name of the file to store the array\n",
    "file_name = \"dependency_model_on.npy\"\n",
    "# Save the array to a .npy file\n",
    "np.save(file_name, w)\n",
    "print(f\"vector weight stored in '{file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file to store the array\n",
    "file_name = \"dependency_model_on.npy\"\n",
    "# Load the array from the file\n",
    "w = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent_id': 'GUM_academic_discrimination-1', 'text': 'The prevalence of discrimination across racial groups in contemporary America:', 'pos_tag': ['DT', 'NN', 'IN', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'JJ', 'NNP', ':'], 'dependacy': {1: {'index': 1, 'original_word': 'The', 'word': 'the', 'pos': 'DT', 'head': 2, 'dependency_relation': 'det'}, 2: {'index': 2, 'original_word': 'prevalence', 'word': 'prevalence', 'pos': 'NN', 'head': 0, 'dependency_relation': 'root'}, 3: {'index': 3, 'original_word': 'of', 'word': 'of', 'pos': 'IN', 'head': 4, 'dependency_relation': 'case'}, 4: {'index': 4, 'original_word': 'discrimination', 'word': 'discrimination', 'pos': 'NN', 'head': 2, 'dependency_relation': 'nmod'}, 5: {'index': 5, 'original_word': 'across', 'word': 'across', 'pos': 'IN', 'head': 7, 'dependency_relation': 'case'}, 6: {'index': 6, 'original_word': 'racial', 'word': 'racial', 'pos': 'JJ', 'head': 7, 'dependency_relation': 'amod'}, 7: {'index': 7, 'original_word': 'groups', 'word': 'group', 'pos': 'NNS', 'head': 2, 'dependency_relation': 'nmod'}, 8: {'index': 8, 'original_word': 'in', 'word': 'in', 'pos': 'IN', 'head': 10, 'dependency_relation': 'case'}, 9: {'index': 9, 'original_word': 'contemporary', 'word': 'contemporary', 'pos': 'JJ', 'head': 10, 'dependency_relation': 'amod'}, 10: {'index': 10, 'original_word': 'America', 'word': 'America', 'pos': 'NNP', 'head': 2, 'dependency_relation': 'nmod'}, 11: {'index': 11, 'original_word': ':', 'word': ':', 'pos': ':', 'head': 2, 'dependency_relation': 'punct'}}}\n",
      "{'sent_id': 'GUM_academic_discrimination-2', 'text': 'Results from a nationally representative sample of adults', 'pos_tag': ['NNS', 'IN', 'DT', 'RB', 'JJ', 'NN', 'IN', 'NNS'], 'dependacy': {1: {'index': 1, 'original_word': 'Results', 'word': 'result', 'pos': 'NNS', 'head': 0, 'dependency_relation': 'root'}, 2: {'index': 2, 'original_word': 'from', 'word': 'from', 'pos': 'IN', 'head': 6, 'dependency_relation': 'case'}, 3: {'index': 3, 'original_word': 'a', 'word': 'a', 'pos': 'DT', 'head': 6, 'dependency_relation': 'det'}, 4: {'index': 4, 'original_word': 'nationally', 'word': 'nationally', 'pos': 'RB', 'head': 5, 'dependency_relation': 'advmod'}, 5: {'index': 5, 'original_word': 'representative', 'word': 'representative', 'pos': 'JJ', 'head': 6, 'dependency_relation': 'amod'}, 6: {'index': 6, 'original_word': 'sample', 'word': 'sample', 'pos': 'NN', 'head': 1, 'dependency_relation': 'nmod'}, 7: {'index': 7, 'original_word': 'of', 'word': 'of', 'pos': 'IN', 'head': 8, 'dependency_relation': 'case'}, 8: {'index': 8, 'original_word': 'adults', 'word': 'adult', 'pos': 'NNS', 'head': 6, 'dependency_relation': 'nmod'}}}\n",
      "{'sent_id': 'GUM_academic_discrimination-3', 'text': 'Introduction.', 'pos_tag': ['NN', '.'], 'dependacy': {1: {'index': 1, 'original_word': 'Introduction', 'word': 'introduction', 'pos': 'NN', 'head': 0, 'dependency_relation': 'root'}, 2: {'index': 2, 'original_word': '.', 'word': '.', 'pos': '.', 'head': 1, 'dependency_relation': 'punct'}}}\n",
      "{'sent_id': 'GUM_academic_discrimination-4', 'text': 'Personal experiences of discrimination and bias have been the focus of much social science research. [1 - 3]', 'pos_tag': ['JJ', 'NNS', 'IN', 'NN', 'CC', 'NN', 'VBP', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'JJ', 'NN', 'NN', '.', '-LRB-', 'CD', 'SYM', 'CD', '-RRB-'], 'dependacy': {1: {'index': 1, 'original_word': 'Personal', 'word': 'personal', 'pos': 'JJ', 'head': 2, 'dependency_relation': 'amod'}, 2: {'index': 2, 'original_word': 'experiences', 'word': 'experience', 'pos': 'NNS', 'head': 10, 'dependency_relation': 'nsubj'}, 3: {'index': 3, 'original_word': 'of', 'word': 'of', 'pos': 'IN', 'head': 4, 'dependency_relation': 'case'}, 4: {'index': 4, 'original_word': 'discrimination', 'word': 'discrimination', 'pos': 'NN', 'head': 2, 'dependency_relation': 'nmod'}, 5: {'index': 5, 'original_word': 'and', 'word': 'and', 'pos': 'CC', 'head': 6, 'dependency_relation': 'cc'}, 6: {'index': 6, 'original_word': 'bias', 'word': 'bias', 'pos': 'NN', 'head': 4, 'dependency_relation': 'conj'}, 7: {'index': 7, 'original_word': 'have', 'word': 'have', 'pos': 'VBP', 'head': 10, 'dependency_relation': 'aux'}, 8: {'index': 8, 'original_word': 'been', 'word': 'be', 'pos': 'VBN', 'head': 10, 'dependency_relation': 'cop'}, 9: {'index': 9, 'original_word': 'the', 'word': 'the', 'pos': 'DT', 'head': 10, 'dependency_relation': 'det'}, 10: {'index': 10, 'original_word': 'focus', 'word': 'focus', 'pos': 'NN', 'head': 0, 'dependency_relation': 'root'}, 11: {'index': 11, 'original_word': 'of', 'word': 'of', 'pos': 'IN', 'head': 15, 'dependency_relation': 'case'}, 12: {'index': 12, 'original_word': 'much', 'word': 'much', 'pos': 'JJ', 'head': 15, 'dependency_relation': 'dep'}, 13: {'index': 13, 'original_word': 'social', 'word': 'social', 'pos': 'JJ', 'head': 14, 'dependency_relation': 'amod'}, 14: {'index': 14, 'original_word': 'science', 'word': 'science', 'pos': 'NN', 'head': 15, 'dependency_relation': 'compound'}, 15: {'index': 15, 'original_word': 'research', 'word': 'research', 'pos': 'NN', 'head': 10, 'dependency_relation': 'nmod'}, 16: {'index': 16, 'original_word': '.', 'word': '.', 'pos': '.', 'head': 10, 'dependency_relation': 'punct'}, 17: {'index': 17, 'original_word': '[', 'word': '[', 'pos': '-LRB-', 'head': 18, 'dependency_relation': 'punct'}, 18: {'index': 18, 'original_word': '1', 'word': '1', 'pos': 'CD', 'head': 10, 'dependency_relation': 'dep'}, 19: {'index': 19, 'original_word': '-', 'word': '-', 'pos': 'SYM', 'head': 20, 'dependency_relation': 'case'}, 20: {'index': 20, 'original_word': '3', 'word': '3', 'pos': 'CD', 'head': 18, 'dependency_relation': 'nmod'}, 21: {'index': 21, 'original_word': ']', 'word': ']', 'pos': '-RRB-', 'head': 18, 'dependency_relation': 'punct'}}}\n"
     ]
    }
   ],
   "source": [
    "# read test.txt to make dataset\n",
    "file_path = 'test.txt'\n",
    "parsed_dataset_test = parse_dataset(file_path)\n",
    "\n",
    "# Print the parsed dataset for inspection\n",
    "for sentence in parsed_dataset_test[:4]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(config, transition, weights):\n",
    "    # Calculate the score for a given transition and configuration\n",
    "    feature_vector = get_feature_vector(config, transition)\n",
    "    return np.dot(weights, feature_vector)\n",
    "\n",
    "# calculate UAS score\n",
    "def calculate_uas(gold_heads, predicted_heads):\n",
    "    total_tokens = len(gold_heads.keys())\n",
    "    correctly_predicted = 0\n",
    "    \n",
    "    for key in gold_heads.keys():\n",
    "        g_head, _ = gold_heads[key]\n",
    "        # print(g_head)\n",
    "        if predicted_heads.get(key) is None:\n",
    "            continue\n",
    "        p_head, _ = predicted_heads[key] \n",
    "        if p_head == g_head:\n",
    "            correctly_predicted += 1\n",
    "    return correctly_predicted / total_tokens\n",
    "\n",
    "# def calculate_uas(gold_arcs, predicted_arcs):\n",
    "#     correct_arcs = 0\n",
    "#     for arc in gold_arcs:\n",
    "#         if arc in predicted_arcs:\n",
    "#             correct_arcs += 1\n",
    "#     return correct_arcs / len(gold_arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_arcs(sentence, weights):\n",
    "    config = Configuration(sentence)\n",
    "    error = False\n",
    "    while len(config.buffer) >= 1: # Continue until buffer is empty\n",
    "        # Determine the gold-standard action using the oracle\n",
    "        gold_action = config.oracle_transition()\n",
    "        # Update configuration based on the gold action\n",
    "        try:\n",
    "            if gold_action == 'LEFT-ARC':\n",
    "                config.do_left_arc()\n",
    "            elif gold_action == 'RIGHT-ARC':\n",
    "                config.do_right_arc()\n",
    "            elif gold_action == 'REDUCE':\n",
    "                config.do_reduce()\n",
    "            elif gold_action == 'SHIFT':\n",
    "                config.do_shift()\n",
    "            else:\n",
    "                print(\"Invalid action for sentence index:\", index)\n",
    "                print(\"Invalid action:\", gold_action)\n",
    "                break\n",
    "        except:\n",
    "            error = True # just to ensure there is no error in original gold tag data\n",
    "            break\n",
    "    dependency_dict = {dep['index']: (head['index'],dep['original_word']) for head, dep in config.arcs}\n",
    "    return dependency_dict, error \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_arcs(sentence, weights):\n",
    "    config = Configuration(sentence)\n",
    "    while len(config.buffer) >= 1: # Continue until buffer is empty\n",
    "        # Determine the gold-standard action using the oracle\n",
    "        t_star = None\n",
    "        max_score = float('-inf')\n",
    "        for transition in ['LEFT-ARC', 'RIGHT-ARC', 'REDUCE', 'SHIFT']:\n",
    "            score = get_score(config, transition, weights)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                t_star = transition\n",
    "\n",
    "        # Update configuration based on the gold action\n",
    "        try:\n",
    "            if len(config.stack) == 0: # heuristic\n",
    "                config.do_shift()\n",
    "            elif t_star == 'LEFT-ARC':\n",
    "                if any(dep['index'] == config.stack[-1]['index'] for head, dep in config.arcs): # heuristic\n",
    "                    config.do_reduce()\n",
    "                else:\n",
    "                    config.do_left_arc()\n",
    "            elif t_star == 'RIGHT-ARC':\n",
    "                config.do_right_arc()\n",
    "            elif t_star == 'REDUCE':\n",
    "                if not any(dep['index'] == config.stack[-1]['index'] for head, dep in config.arcs): # heuristic\n",
    "                    config.do_left_arc()\n",
    "                else:\n",
    "                    config.do_reduce()\n",
    "            elif t_star == 'SHIFT':\n",
    "                config.do_shift()\n",
    "            else:\n",
    "                print(\"Invalid action for sentence index:\", index)\n",
    "                print(\"Invalid action:\", t_star)\n",
    "                break\n",
    "        except:\n",
    "            \n",
    "            break\n",
    "    # dependency_list = [ (head['original_word'],dep['original_word']) for head, dep in config.arcs ]\n",
    "    dependency_dict = {dep['index']: (head['index'],dep['original_word']) for head, dep in config.arcs}\n",
    "    return dependency_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(sentences, weights):\n",
    "    # Predict transitions for each sentence in the test data\n",
    "    predicted_arcs = []\n",
    "    error = 0\n",
    "    score = 0\n",
    "    total_len = 0\n",
    "    predictions_output = []\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        \n",
    "        gold_heads, error = gold_arcs(sentence, weights)\n",
    "        predicted_heads = predict_arcs(sentence, weights)\n",
    "        # print(gold_heads, predicted_heads)\n",
    "        if not error: # just to ensure there is no error in original gold tag data\n",
    "            total_len += 1\n",
    "            score += calculate_uas(gold_heads, predicted_heads)\n",
    "            # Save predictions to the list\n",
    "            predictions_output.append((sentence['sent_id'], predicted_heads))\n",
    "    return score / total_len , predictions_output , total_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test data\n",
    "test_acc, predictions_output, data_len = predict_test(parsed_dataset_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 100 test data: 71.8457%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for {data_len} test data: {test_acc*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to dependency_predictions_on.tsv\n"
     ]
    }
   ],
   "source": [
    "# output file write \n",
    "output_file_path =  \"dependency_predictions_on.tsv\"\n",
    "with open(output_file_path, \"w\", encoding='utf-8') as output_file:\n",
    "    # Write the header\n",
    "    output_file.write(\"sent_id\\ttoken_number\\tword\\tpredicted_head_index\\n\")\n",
    "\n",
    "    # Write the predictions\n",
    "    for sent_id, pred_head_dict in predictions_output:\n",
    "        for token, arc in pred_head_dict.items():\n",
    "            head, word = arc\n",
    "            output_file.write(f\"{sent_id}\\t{token}\\t{word}\\t{head}\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    print(f\"Predictions saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
