# NLP Project: POS Tagging and Dependency Parsing

## Overview
This project focuses on two key tasks in Natural Language Processing (NLP): **Part-of-Speech (POS) Tagging** and **Dependency Parsing**. The project was developed as part of an assignment for the course *Natural Language Processing (CS60075)*.

### Task 1: POS Tagging
The objective of this task is to implement the Viterbi Algorithm to assign POS tags to words in a sentence. The project uses the **UD-English-GUM** dataset, where the model calculates transition and emission probabilities from the training data to predict tags on test data.

### Task 2: Dependency Parsing
In this task, a dependency parser is designed to generate dependency graphs for sentences. The parser is trained using a linear classifier and evaluated on the **Unlabeled Attachment Score (UAS)** metric.

## Features
- **Viterbi Algorithm Implementation**: Calculates transition and emission probabilities for POS tagging.
- **Add-One Smoothing**: Handles unseen words during POS tagging.
- **Dependency Parsing**: Uses an Arc-Eager Parsing algorithm with a linear classifier.
- **Evaluation Metrics**: Includes accuracy, precision, recall, F1-score, and UAS.

## Viterbi Algorithm in NLP
The Viterbi algorithm is a dynamic programming algorithm used in Natural Language Processing (NLP) for tasks like Part-of-Speech (POS) tagging. In POS tagging, the goal is to assign a grammatical category (like noun, verb, adjective) to each word in a sentence.

### How It Works:
- **Hidden Markov Model (HMM)**: The algorithm assumes that the sequence of words is generated by a Markov process, where each word's POS tag depends on the previous tag. This is modeled using a Hidden Markov Model (HMM).
- **Transition and Emission Probabilities**: The Viterbi algorithm computes the most probable sequence of tags by considering:
  - **Transition probabilities**: The likelihood of moving from one tag to another.
  - **Emission probabilities**: The likelihood of a word being a specific tag.
- **Dynamic Programming**: It builds a matrix (the Viterbi matrix) where each cell represents the probability of a certain tag sequence up to a certain word. By backtracking through this matrix, the algorithm finds the most probable sequence of tags for the entire sentence.

## Dependency Parser in NLP
A dependency parser in NLP is used to analyze the grammatical structure of a sentence by establishing relationships between "head" words and words that are dependent on them. This relationship is depicted in the form of a **dependency graph**.

### How It Works:
- **Dependency Relations**: Each word in a sentence is connected to another word (its head) with a directed arc, representing a grammatical relationship (like subject, object).
- **Parsing Algorithms**: The parser uses algorithms (like the Arc-Eager Parsing algorithm) to process a sentence, typically by maintaining:
  - **Stack (S)**: Holds words that have been partially processed.
  - **Buffer (B)**: Contains words yet to be processed.
  - **Set of Arcs (A)**: Contains the dependency relationships (head-dependent pairs) established so far.
- **Classifiers**: A classifier, often trained on annotated data, is used to predict which action (like attaching a word to its head) to perform at each step, transitioning between different configurations until the entire sentence is parsed.
- **Output**: The final output is a dependency graph where each word is connected to its head, showing the structure of the sentence.

## Installation

### Prerequisites
- Python 3.x
- Numpy

### Setup
1. Clone this repository:
    ```bash
    git https://github.com/Gourav-s12/Viterbi-and-Dependency-Parser-NLP
    cd Viterbi-and-Dependency-Parser-NLP
    ```
2. Install the required packages:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

### Task 1: POS Tagging
1. **Train the Model**:
    ```bash
    python src/posViterbi.py --mode train
    ```
2. **Test the Model**:
    ```bash
    python src/posViterbi.py --mode test
    ```
3. **View Predictions**:
    The predictions are saved as TSV files.

### Task 2: Dependency Parsing
1. **Train the Model**:
    ```bash
    python src/dependency.py --mode train
    ```
2. **Test the Model**:
    ```bash
    python src/dependency.py --mode test
    ```
3. **View Predictions**:
    The predictions are saved as TSV files.

## Evaluation

- **POS Tagging**: Evaluate the model on accuracy, precision, recall, and F1-score.
- **Dependency Parsing**: Evaluate the model using the UAS metric.

## Deliverables

- **POS Tagging**:
  - Python script: `posViterbi.py`
  - Report: `viterbi_report.pdf`
  - Predictions: `viterbi_predictions_train.tsv`, `viterbi_predictions_test.tsv`
  
- **Dependency Parsing**:
  - Python script: `dependency.py`
  - Report: `dependency_report.pdf`
  - Model Weights: `dependency_model_on.npy`
  - Predictions: `dependency_predictions_on.tsv`

## Contributing
To contribute, fork the repository, create a new branch, make your changes, and open a pull request.

## License
This project is licensed under the MIT License.

## Acknowledgments
- Course: *Natural Language Processing (CS60075)*
- Dataset: [UD-English-GUM](https://drive.google.com/file/d/1Pc8FU-VdDP8tZIR1V0PxCmiUYFAE1-d5/view?usp=sharing)
